#!/usr/bin/env python3
"""
Test Monitoring and Logging System
==================================

This script tests the comprehensive monitoring infrastructure including:
- Agent logging with structured data
- Metrics collection and analysis
- Execution monitoring with real-time alerts
- Health checking across all components
"""

import os
import sys
import time
import json
from pathlib import Path
from datetime import datetime

# Add parent directory to path
sys.path.append(str(Path(__file__).parent))

# Load environment variables
from dotenv import load_dotenv
load_dotenv('.env.local')

from agent_protocol.monitoring import (
    get_agent_logger, get_metrics_collector, 
    get_execution_monitor, get_health_checker
)
from agent_protocol.executors.agent_executor import AgentExecutor
from agent_protocol.agents import InventoryMonitorAgent
from agent_protocol.core.agent_types import AgentType


def test_logging_system():
    """Test structured logging system."""
    print("\n" + "="*60)
    print("üìù AGENT LOGGING SYSTEM TEST")
    print("="*60)
    
    logger = get_agent_logger()
    
    # Test manual logging
    print("\nüîç Testing manual logging...")
    
    with logger.execution_context(
        "test_agent_001", AgentType.INVENTORY_MONITOR, 
        "test_exec_001", "user_123", "org_456"
    ):
        logger.log_tool_call("database_query", {"table": "inventory"}, {"rows": 150})
        logger.log_llm_call("openai", "gpt-4", 1250, 1500, 0.025)
        logger.log_agent_decision("reorder_recommended", 0.92, ["Low stock detected", "Lead time exceeded"])
        logger.log_performance_metric("response_time", 850.5, "ms")
    
    # Get recent logs
    recent_logs = logger.get_recent_logs(10)
    print(f"‚úÖ Generated {len(recent_logs)} log entries")
    
    for log in recent_logs[-3:]:
        print(f"   üìã {log.event_type}: {log.message}")
    
    # Test execution summary
    summary = logger.get_execution_summary("test_exec_001")
    print(f"\nüìä Execution Summary:")
    print(f"   ‚Ä¢ Status: {summary['status']}")
    print(f"   ‚Ä¢ Tool calls: {summary['tool_calls_count']}")
    print(f"   ‚Ä¢ LLM calls: {summary['llm_calls_count']}")
    print(f"   ‚Ä¢ Total tokens: {summary['total_tokens']}")
    print(f"   ‚Ä¢ Total cost: ${summary['total_cost']:.4f}")


def test_metrics_collection():
    """Test metrics collection system."""
    print("\n\n" + "="*60)
    print("üìä METRICS COLLECTION SYSTEM TEST")
    print("="*60)
    
    metrics = get_metrics_collector()
    
    # Simulate multiple agent executions
    print("\nüîÑ Simulating agent executions...")
    
    for i in range(5):
        agent_id = f"test_agent_{i % 3}"
        agent_type = [AgentType.INVENTORY_MONITOR, AgentType.SUPPLIER_EVALUATOR, AgentType.DEMAND_FORECASTER][i % 3]
        execution_id = f"exec_{i}_{int(time.time())}"
        
        # Start execution
        metrics.record_execution_start(agent_id, agent_type, execution_id)
        
        # Simulate work
        time.sleep(0.1)
        
        # Simulate LLM calls
        metrics.record_llm_call(agent_id, agent_type, "openai", "gpt-4", 500 + i * 100, 1000 + i * 200, 0.01 + i * 0.005)
        
        # Simulate tool calls
        metrics.record_tool_call(agent_id, agent_type, "database_query", True, 50 + i * 10)
        
        # End execution
        metrics.record_execution_end(agent_id, agent_type, execution_id, True, 150 + i * 50, 0.8 + i * 0.02)
    
    print(f"‚úÖ Simulated 5 agent executions")
    
    # Get system metrics
    system_metrics = metrics.get_system_metrics()
    print(f"\nüìà System Metrics:")
    print(f"   ‚Ä¢ Total agents: {system_metrics['total_agents']}")
    print(f"   ‚Ä¢ Active agents: {system_metrics['active_agents']}")
    print(f"   ‚Ä¢ Executions today: {system_metrics['total_executions_today']}")
    print(f"   ‚Ä¢ Cost today: ${system_metrics['total_cost_today']:.4f}")
    
    # Get agent-specific metrics
    agent_metrics = metrics.get_all_agent_metrics()
    print(f"\nü§ñ Agent Metrics ({len(agent_metrics)} agents):")
    
    for agent_id, agent_data in list(agent_metrics.items())[:3]:
        print(f"\n   Agent {agent_id}:")
        print(f"   ‚Ä¢ Executions: {agent_data.total_executions}")
        print(f"   ‚Ä¢ Success rate: {agent_data.success_rate:.1f}%")
        print(f"   ‚Ä¢ Avg execution time: {agent_data.avg_execution_time_ms:.1f}ms")
        print(f"   ‚Ä¢ Total tokens: {agent_data.total_tokens_used}")
        print(f"   ‚Ä¢ Total cost: ${agent_data.total_llm_cost:.4f}")
    
    # Test cost analysis
    cost_analysis = metrics.get_cost_analysis()
    print(f"\nüí∞ Cost Analysis:")
    print(f"   ‚Ä¢ Total cost (24h): ${cost_analysis['total_cost']:.4f}")
    print(f"   ‚Ä¢ Avg cost per agent: ${cost_analysis['avg_cost_per_agent']:.4f}")
    
    # Test usage patterns
    usage_patterns = metrics.get_usage_patterns()
    print(f"\n‚è∞ Usage Patterns:")
    print(f"   ‚Ä¢ Peak hour: {usage_patterns['peak_hour']}")
    print(f"   ‚Ä¢ Total executions: {usage_patterns['total_executions']}")


def test_execution_monitoring():
    """Test real-time execution monitoring."""
    print("\n\n" + "="*60)
    print("üëÅÔ∏è  EXECUTION MONITORING SYSTEM TEST")
    print("="*60)
    
    monitor = get_execution_monitor()
    
    # Test alert callback
    alerts_received = []
    def alert_callback(alert):
        alerts_received.append(alert)
        print(f"   üö® Alert: {alert.level.value.upper()} - {alert.title}")
    
    monitor.add_alert_callback(alert_callback)
    
    # Simulate monitored execution
    print("\nüöÄ Starting monitored execution...")
    
    execution_state = monitor.start_execution(
        "monitor_test_001", "test_agent_monitor", AgentType.INVENTORY_MONITOR
    )
    
    print(f"‚úÖ Execution started: {execution_state.execution_id}")
    print(f"   ‚Ä¢ Agent: {execution_state.agent_id}")
    print(f"   ‚Ä¢ Status: {execution_state.status}")
    
    # Simulate progress updates
    time.sleep(0.1)
    monitor.update_execution(execution_state.execution_id, "analyzing_data", 0.3, tokens_used=150)
    print(f"   ‚Ä¢ Progress: 30% - analyzing data")
    
    time.sleep(0.1)
    monitor.record_tool_call(execution_state.execution_id, "database_query", True)
    monitor.update_execution(execution_state.execution_id, "generating_insights", 0.7, tokens_used=200)
    print(f"   ‚Ä¢ Progress: 70% - generating insights")
    
    time.sleep(0.1)
    monitor.update_execution(execution_state.execution_id, "finalizing", 0.9, tokens_used=100)
    print(f"   ‚Ä¢ Progress: 90% - finalizing")
    
    # Complete execution
    monitor.end_execution(execution_state.execution_id, True, 450)
    print(f"‚úÖ Execution completed successfully")
    
    # Get execution state
    final_state = monitor.get_execution_state(execution_state.execution_id)
    if final_state:
        print(f"\nüìä Final Execution State:")
        print(f"   ‚Ä¢ Duration: {final_state.duration_ms}ms")
        print(f"   ‚Ä¢ Tokens used: {final_state.tokens_used}")
        print(f"   ‚Ä¢ Tool calls: {final_state.tool_calls}")
        print(f"   ‚Ä¢ Status: {final_state.status}")
    
    # Test system health
    system_health = monitor.get_system_health()
    print(f"\nüè• System Health:")
    print(f"   ‚Ä¢ Status: {system_health['status']}")
    print(f"   ‚Ä¢ Active executions: {system_health['active_executions']}")
    print(f"   ‚Ä¢ Recent alerts: {system_health['unresolved_alerts']}")
    
    # Show alerts if any
    if alerts_received:
        print(f"\nüö® Alerts Generated: {len(alerts_received)}")
        for alert in alerts_received:
            print(f"   ‚Ä¢ {alert.level.value}: {alert.message}")


def test_health_checking():
    """Test comprehensive health checking."""
    print("\n\n" + "="*60)
    print("üè• HEALTH CHECKING SYSTEM TEST")
    print("="*60)
    
    health_checker = get_health_checker()
    
    print("\nüîç Performing comprehensive health check...")
    
    # Force immediate health check
    health_results = health_checker.force_health_check()
    
    print(f"‚úÖ Health check completed for {len(health_results)} components")
    
    # Show overall status
    overall_status = health_checker.get_overall_status()
    print(f"\nüìä Overall System Status: {overall_status.value.upper()}")
    
    # Show component details
    print(f"\nüîß Component Health:")
    for component, check in health_results.items():
        status_emoji = {
            "healthy": "‚úÖ",
            "degraded": "‚ö†Ô∏è",
            "unhealthy": "‚ùå",
            "unknown": "‚ùì"
        }.get(check.status.value, "‚ùì")
        
        print(f"   {status_emoji} {component}: {check.message}")
        
        if check.response_time_ms:
            print(f"      ‚è±Ô∏è  Response time: {check.response_time_ms}ms")
        
        if check.metadata:
            interesting_metrics = {}
            metadata = check.metadata
            
            # Extract interesting metrics
            if 'cpu_percent' in metadata:
                interesting_metrics['CPU'] = f"{metadata['cpu_percent']:.1f}%"
            if 'memory_percent' in metadata:
                interesting_metrics['Memory'] = f"{metadata['memory_percent']:.1f}%"
            if 'query_time_ms' in metadata:
                interesting_metrics['DB Query'] = f"{metadata['query_time_ms']:.1f}ms"
            if 'total_providers' in metadata:
                interesting_metrics['LLM Providers'] = f"{metadata['healthy_providers']}/{metadata['total_providers']}"
            
            if interesting_metrics:
                metrics_str = ", ".join(f"{k}: {v}" for k, v in interesting_metrics.items())
                print(f"      üìä {metrics_str}")
    
    # Get health summary
    health_summary = health_checker.get_health_summary()
    component_counts = health_summary['component_count']
    
    print(f"\nüìà Health Summary:")
    print(f"   ‚Ä¢ Total components: {component_counts['total']}")
    print(f"   ‚Ä¢ Healthy: {component_counts['healthy']}")
    print(f"   ‚Ä¢ Degraded: {component_counts['degraded']}")
    print(f"   ‚Ä¢ Unhealthy: {component_counts['unhealthy']}")
    print(f"   ‚Ä¢ Unknown: {component_counts['unknown']}")


def test_full_agent_with_monitoring():
    """Test full agent execution with monitoring integration."""
    print("\n\n" + "="*60)
    print("ü§ñ FULL AGENT WITH MONITORING TEST")
    print("="*60)
    
    # Create executor
    executor = AgentExecutor(max_workers=1, default_timeout=60)
    executor.register_agent_class(AgentType.INVENTORY_MONITOR, InventoryMonitorAgent)
    
    # Create agent
    agent = executor.create_agent(
        agent_id="monitor_test_agent",
        agent_type=AgentType.INVENTORY_MONITOR,
        name="Monitoring Test Agent",
        description="Agent for testing monitoring integration",
        config={
            "critical_threshold_days": 3,
            "warning_threshold_days": 7
        }
    )
    
    print(f"\n‚úÖ Created agent: {agent.name}")
    
    # Execute agent with monitoring
    print(f"\nüöÄ Executing agent with full monitoring...")
    
    result = executor.execute_agent(
        agent_id="monitor_test_agent",
        input_data={
            "action": "monitor",
            "scope": "critical_items"
        },
        org_id="test_org",
        user_id="test_user"
    )
    
    if result.success:
        print(f"‚úÖ Agent execution successful!")
        print(f"   ‚Ä¢ Execution ID: {result.execution_id}")
        print(f"   ‚Ä¢ Duration: {result.execution_time_ms}ms")
        print(f"   ‚Ä¢ Tokens used: {result.tokens_used}")
        print(f"   ‚Ä¢ Confidence: {result.confidence:.2f}")
        
        # Show monitoring data
        logger = get_agent_logger()
        execution_summary = logger.get_execution_summary(result.execution_id)
        
        print(f"\nüìä Monitoring Summary:")
        print(f"   ‚Ä¢ Status: {execution_summary['status']}")
        print(f"   ‚Ä¢ Duration: {execution_summary.get('duration_ms', 'unknown')}ms")
        print(f"   ‚Ä¢ Tool calls: {execution_summary['tool_calls_count']}")
        print(f"   ‚Ä¢ LLM calls: {execution_summary['llm_calls_count']}")
        print(f"   ‚Ä¢ Total cost: ${execution_summary['total_cost']:.4f}")
        
    else:
        print(f"‚ùå Agent execution failed: {result.message}")


def test_monitoring_export():
    """Test monitoring data export capabilities."""
    print("\n\n" + "="*60)
    print("üì§ MONITORING EXPORT TEST")
    print("="*60)
    
    metrics = get_metrics_collector()
    
    # Export comprehensive report
    print("\nüìÑ Generating comprehensive metrics report...")
    report_path = metrics.export_report()
    
    print(f"‚úÖ Report exported to: {report_path}")
    
    # Show report summary
    if Path(report_path).exists():
        with open(report_path, 'r') as f:
            report_data = json.load(f)
        
        print(f"\nüìä Report Summary:")
        print(f"   ‚Ä¢ Generated at: {report_data['generated_at']}")
        print(f"   ‚Ä¢ System metrics included: ‚úÖ")
        print(f"   ‚Ä¢ Agent metrics count: {len(report_data.get('agent_metrics', {}))}")
        print(f"   ‚Ä¢ Cost analysis included: ‚úÖ")
        print(f"   ‚Ä¢ Usage patterns included: ‚úÖ")
        
        file_size = Path(report_path).stat().st_size
        print(f"   ‚Ä¢ Report size: {file_size:,} bytes")


def main():
    """Run comprehensive monitoring system tests."""
    print("üöÄ MONITORING SYSTEM TESTING SUITE")
    print("=" * 60)
    print(f"üïê Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # Test 1: Logging system
        test_logging_system()
        
        # Test 2: Metrics collection
        test_metrics_collection()
        
        # Test 3: Execution monitoring
        test_execution_monitoring()
        
        # Test 4: Health checking
        test_health_checking()
        
        # Test 5: Full agent integration
        test_full_agent_with_monitoring()
        
        # Test 6: Export capabilities
        test_monitoring_export()
        
        print("\n\n‚úÖ MONITORING SYSTEM TESTS COMPLETED!")
        print("=" * 60)
        
        print("\nüéâ All monitoring components are working correctly!")
        print("\nüìã Components tested:")
        print("   ‚úÖ Structured logging with execution context")
        print("   ‚úÖ Comprehensive metrics collection")
        print("   ‚úÖ Real-time execution monitoring")
        print("   ‚úÖ Multi-component health checking")
        print("   ‚úÖ Full agent integration")
        print("   ‚úÖ Data export and reporting")
        
    except Exception as e:
        print(f"\n‚ùå Test suite failed: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()